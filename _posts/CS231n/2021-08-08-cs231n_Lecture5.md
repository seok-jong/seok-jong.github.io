---
title: "[CS231n] Lecture 5"
categories:
  - AI
tags:
  - AI
  - CS231n
"toc": true
"toc_sticky": true

---

# [CS231n] Lecture 5 | Convolutional Neural Networks

## 1. Convolutional Neural Networks

이번 강의에서는 CNN에대한 간단한 동작 방식과 Fully Connected Layer에 대한 차이를 알아본다. 

먼저 지난 강의에서 배운 FC Layer부터 살펴본다. ![Screenshot from 2021-08-08 13-13-11](https://user-images.githubusercontent.com/77032455/128620481-568aa516-78c3-40c3-95ba-63b352975ae0.png)

FC layer에서는 (32 x 32 x 3)의 이미지를 3072 길이의 벡터로 펴서 가중치에 곱하는 방식으로 활성화값을 추출한다. 

![Screenshot from 2021-08-08 13-17-21](https://user-images.githubusercontent.com/77032455/128620578-64b9b7e2-c677-4e84-96b0-16eb3020e47e.png)

그에 반해 CNN은 입력 이미지의 데이터를 변환하지 않고 그대로 이용하는 것이  FC layer와 가장 큰 차이점이다. 또한 FC layer에 비해 parameter의 갯수가 크게 줄어든다는 특징이 있다. 
CNN에서의 parameter는 filter의 크기이기 때문이다. 

이미지의 형태를 그대로 보존함으로써 픽셀들 간의 공간정보를 보존할 수 있다. 
이때, filter를 곱하여 활성화값을 추출하는데 위 이미지에서 보듯이 filter가 가중치의 역할을 하여 입력데이터 위에서 미끄러져가며 곱해지면서 활성화값을 추출하게 된다. 

1개의 filter로 CNN을 수행하게 되면 1개의 채널을 가진 활성화값이 추출되고 N개의 filter로 CNN을 수행하게 되면 N개의 채널을 가진 활성화값이 생성되는 것이다. 

filter가 곱해질 때는 입력 데이터의 filter가 위치한 곳의 데이터와 픽셀곱을 수행한다. (실제로는 1차원으로 펴서 곱한다고 함)
filter가 5x5x3의 크기를 가지고 있으면 데이터의 5x5x3의 크기를 가진 데이터와 픽셀곱을 하게 되는 것이다. 따라서 활성화값의 채널 크기가 1로 되는 것이다. 



![Screenshot from 2021-08-08 13-28-55](https://user-images.githubusercontent.com/77032455/128620807-1441ae73-b057-46c1-ae43-d2895cd5891c.png)

각각의 filter들을 가중치의 역할을 한다고 위에서 언급하였다. 따라서 backpropagation을 통해 학습되고 업데이트되는 것은 바로 이 filter들인데 이 값들은 이전 강의에서 보았듯이 입력 데이터와 닮은 형태로 점점 업데이트 된다. 따라서 활성화 값도 더욱 활성화 되는 것이다. 

이러한 filter를 통해 계산된 활성화값을 시각화한 것이 위 이미지이다. 하얀색부분이 더 많을수록 데이터가 filter에 대해서 더욱 활성화 되었다고 볼 수 있다. 
빨간 박스의 부분은 활성화가 적게 된 예시이고 파란색 박스는 활성화가 많이된 예시이다. 

개인적인 생각을 말하자면 
빨간 박스의 경우 filter의 형태를 보아 edge를 찾는다고 생각한다. 하지만 데이터로 제시된 이미지에 오른쪽으로 증가하는 형태의 edge는 뚜렷하게 구분되지 않는다. 따라서 적게 활성화되었다고 생각하고, 파란색 박스의 경우 filter를 보면 주황색을 찾는다고 생각한다. 따라서 입력이미지의 주황색부분이 우측 상단에 있으므로, activation map에서도 우측 상단에서 더욱 활성화 된 모습이 나타난다고 생각한다. 



## 2. Spatial Dimension

![Screenshot from 2021-08-08 13-40-06](https://user-images.githubusercontent.com/77032455/128621013-dda994e1-3d3a-4519-941e-bb862c46389a.png)

CNN을 통한 계산 방법에 대해서 알기 위해서는 먼저 3가지 용어에 대해서 알아야 한다. 

1. Filter size 
2. Stride
3. Padding

Filter size는 위에서 보았듯이 입력데이터 위에서 미끄러지는 filter의 크기를 말한다. 여기에서는 한 변의 길이를 이용한다.( 보통 정사각형 )

Stride는 filter가 데이터위에서 이동하는 크기를 뜻한다. stride가 1이면 좌측상단에서 우측상단으로 이동할때, 1칸 이동하여 filter가 적용됨을 의미한다.

Padding은 위 이미지에서 회색으로 표시된 부분이며 입력데이터의 테두리에 임의의 값을 채워넣는 것이다. Padding을 하는 이유는 크게 2가지이다. 첫번째로 유동적으로 변하는 filter의 크기와 stride에 대해서 우리가 원하는 activation 값의 크기를 조절하기 위해서이고, 두번째로 CNN의 작동방식 특성상 테두리 부분은 활성화값에 충분히 반영되지 않는 특징이 있기 때문에 이를 해결하기 위해 사용한다. ( 가장 테두리 부분은 1번씩만 곱해지기 때문)

이를 이용하여 출력값의 크기를 예측하는 공식은 다음과 같다. 

![Screenshot from 2021-08-08 13-47-40](https://user-images.githubusercontent.com/77032455/128621119-1211e4ef-9ce4-4f2d-95ee-46eb0655edb7.png)

N은 입력이미지의 크기이다. Padding이 적용될 경우 입력이미지가 커진다고 생각하면 된다. Padding이 1로 적용이되면 입력이미지의 한 변의 길이는 2가 늘어난다.( 양쪽으로 적용되니까)따라서 N에 2를 더해줘서 곱해주면 된다. F에 filter 의 크기 , stride에 stride의 크기를 입력해서 계산하면 출력 값의 크기를 예측할 수 있다. 

이때, 출력값의 channel은 적용된 filter의 수 임을 잊지 않아야 한다.



## 3. Pooling layer

![Screenshot from 2021-08-08 13-56-12](https://user-images.githubusercontent.com/77032455/128621264-47a93c4e-ec5c-4a00-adf3-2d61474d3cc8.png)

pooling layer는 활성화맵의 크기를 줄이는 방식을 뜻한다. 이를 Down Sampling한다고 하는데  그냥 줄이는 형식이 아닌 공간적으로 줄이는 방식이다. depth에 대해서는 줄이지 않는다.
pooling layer를 이용해 얻는 이점으로는 파라미터의 수를 줄일수 있다는 점이다. 또한 공간적인 불변성도 얻을 수 있다고 한다.

![Screenshot from 2021-08-08 13-59-55](https://user-images.githubusercontent.com/77032455/128621351-3c077507-fb9f-4caa-9b12-09672337fcd8.png)

일반적으로 MAX Pooling이 자주쓰인다고 한다. 
이름에서 알 수 있듯이, CNN을 적용하는 것과 유사하게 입력데이터에서 filter가 미끄러져가며 계산되는 방식이다. 다만 픽셀곱이 아닌 max함수를 실행하게 된다. 위 이미지에서 보이듯이 각 색의 칸이 filter가 적용된 Receptive Field를 의미하는데 이 부분에서 max값을 추출하여 활성화값으로 추출하는 것이다. 

최근에는 max pooling을 진핼할 때, 전용 method를 사용하는 것이 아니라 CNN을 이용하여 계산하는 것이 유행(?)이라고 한다. 





# Reference

https://www.youtube.com/watch?v=bNb2fEVKeEo&list=RDCMUCdKG2JnvPu6mY1NDXYFfN0g&index=3

http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf

https://lynnshin.tistory.com/7?category=919376